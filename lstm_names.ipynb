{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementing LSTM Neural Network for Serbian name generation\n",
    "Dataset: srpska-imena <br>\n",
    "Link: https://github.com/fondacija-glasnik/srpska-imena"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import required modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from functools import reduce"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = r\"SrpskaImena.csv\"\n",
    "data = pd.read_csv(dataset_path)\n",
    "names = data['Name'].to_list()\n",
    "names = [name.lower() for name in names]\n",
    "\n",
    "names = np.array(names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Shape = (815,)\n",
      "\n",
      "Lets see some names: \n",
      "['antonije' 'arandjel' 'arsenije' 'atanasije' 'acim' 'aca' 'aco' 'adam'\n",
      " 'aksentije']\n"
     ]
    }
   ],
   "source": [
    "print(f\"Data Shape = {names.shape}\")\n",
    "print()\n",
    "print(\"Lets see some names: \")\n",
    "print(names[1:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transform the data\n",
    "Find the longest name and append dots ('.') to every other name until its length matches the longest one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['avram......' 'antonije...' 'arandjel...' 'arsenije...' 'atanasije..'\n",
      " 'acim.......' 'aca........' 'aco........' 'adam.......' 'aksentije..']\n"
     ]
    }
   ],
   "source": [
    "longest_name_len = max(list(map(lambda x: len(x), names)))\n",
    "transformed_names = np.array(list(map(lambda name: name + '.'*(longest_name_len - len(name)), names)))\n",
    "transformed_names = transformed_names\n",
    "\n",
    "print(transformed_names[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's create the vocabulary based on our dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size = 25\n",
      "Vocab      = {'c', '.', 'r', 'l', 'n', 'f', 'i', 'b', 'a', 'm', 'v', 'u', 'p', '|', 'j', 'h', ' ', 's', 'o', 'z', 'g', 'e', 'k', 't', 'd'}\n"
     ]
    }
   ],
   "source": [
    "vocab = []\n",
    "for name in transformed_names:\n",
    "    vocab.extend(list(name))\n",
    "vocab = set(vocab)\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "print(f\"Vocab size = {vocab_size}\")\n",
    "print(f\"Vocab      = {vocab}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Character to idx (and vice-versa) mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a-8, 8-a\n"
     ]
    }
   ],
   "source": [
    "char_to_idx = {c:i for i, c in enumerate(vocab)}\n",
    "idx_to_char = {i:c for i, c in enumerate(vocab)}\n",
    "\n",
    "a_idx = char_to_idx['a']\n",
    "print(f\"a-{a_idx}, 8-{idx_to_char[a_idx]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We'll split the dataset into batches of a certain size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array(['avram......', 'antonije...', 'arandjel...', 'arsenije...',\n",
      "       'atanasije..', 'acim.......', 'aca........', 'aco........',\n",
      "       'adam.......', 'aksentije..', 'aleksa.....', 'aleksandar.',\n",
      "       'alimpije...', 'andjelko...', 'andrija....', 'bane.......'],\n",
      "      dtype='<U11'), array(['blagoje....', 'blagomir...', 'blaza......', 'bogdan.....',\n",
      "       'bogoljub...', 'bogomir....', 'bogosav....', 'borko......',\n",
      "       'bozidar....', 'bojan......', 'borivoje...', 'borisav....',\n",
      "       'borislav...', 'bosko......', 'branimir...', 'bratislav..'],\n",
      "      dtype='<U11')]\n"
     ]
    }
   ],
   "source": [
    "X_train = []\n",
    "batch_size = 16\n",
    "\n",
    "for i in range(len(transformed_names)//batch_size):\n",
    "    X_train.append(transformed_names[i*batch_size:(i+1)*batch_size])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "#number of input units or embedding size\n",
    "input_units = 64\n",
    "\n",
    "#number of hidden neurons\n",
    "hidden_units = 128\n",
    "\n",
    "#number of output units i.e vocab size\n",
    "output_units = vocab_size\n",
    "\n",
    "#learning rate\n",
    "learning_rate = 0.005"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
