{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementing LSTM Neural Network for Serbian name generation\n",
    "Dataset: srpska-imena <br>\n",
    "Link: https://github.com/fondacija-glasnik/srpska-imena"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import required modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 422,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 423,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data size: 30741\n"
     ]
    }
   ],
   "source": [
    "dataset_path = \"the_sopranos_pilot.txt\"\n",
    "with open(dataset_path, 'r') as file:\n",
    "    data = file.read()\n",
    "\n",
    "# data = data.lower()\n",
    "\n",
    "print(f\"Data size: {len(data)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's create the vocabulary based on our dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 424,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size = 76\n",
      "Vocab      = {'T', ':', 'M', 'l', '(', '7', '-', 'n', '1', 'D', 'v', 'G', 'o', 'S', '^', 'b', 'U', 'B', 'j', 'O', '0', 'e', ';', 'F', 'E', '6', 'k', '4', 'y', ',', 'u', 'p', 'C', 'w', '\\n', '.', 'L', 'i', '3', '5', 'W', 'h', 'X', 'Q', 'I', 'd', 'm', '!', '9', 'a', 't', 'J', 'f', 'V', 'R', \"'\", '?', '/', 'r', '[', 'A', 'Y', 'H', 'c', 'K', '\"', ' ', 'P', 'g', 'x', '2', 'q', 'z', 'N', 's', ')'}\n"
     ]
    }
   ],
   "source": [
    "vocab = set(data)\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "print(f\"Vocab size = {vocab_size}\")\n",
    "print(f\"Vocab      = {vocab}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Character to idx (and vice-versa) mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 425,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a-49, 49-a\n"
     ]
    }
   ],
   "source": [
    "char_to_idx = {c:i for i, c in enumerate(vocab)}\n",
    "idx_to_char = {i:c for i, c in enumerate(vocab)}\n",
    "\n",
    "a_idx = char_to_idx['a']\n",
    "print(f\"a-{a_idx}, {a_idx}-{idx_to_char[a_idx]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We'll split the dataset into batches of a certain size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 426,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_size = len(data)\n",
    "X_train = data[:-1]\n",
    "Y_train = data[1:]\n",
    "\n",
    "# X_test = data[int(0.8*data_size)-1:-1]\n",
    "# Y_test = data[int(0.8*data_size):]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activation functions and their derivatives\n",
    "Tanh: https://en.wikipedia.org/wiki/Hyperbolic_functions#Tanh <br>\n",
    "Sigmoid: https://en.wikipedia.org/wiki/Sigmoid_function <br>\n",
    "Softmax: https://en.wikipedia.org/wiki/Softmax_function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 428,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tanh(x: np.ndarray) -> np.ndarray:\n",
    "    return np.tanh(x)\n",
    "def dtanh(x: np.ndarray) -> np.ndarray:\n",
    "    return 1 - np.tanh(x)**2\n",
    "\n",
    "def sigmoid(x: np.ndarray) -> np.ndarray:\n",
    "    return 1/(1+np.exp(-x))\n",
    "def dsigmoid(x: np.ndarray) -> np.ndarray:\n",
    "    return sigmoid(x)*(1-sigmoid(x))\n",
    "\n",
    "def softmax(x: np.ndarray) -> np.ndarray:\n",
    "    exp = np.exp(x)\n",
    "    return exp/np.sum(exp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper functions\n",
    "One-hot encoding: https://en.wikipedia.org/wiki/One-hot <br>\n",
    "Weight initialization: https://machinelearningmastery.com/weight-initialization-for-deep-learning-neural-networks/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 429,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encode(c: str) -> np.ndarray:\n",
    "    one_hot = np.zeros((vocab_size, 1))\n",
    "    one_hot[char_to_idx[c]] = 1\n",
    "    \n",
    "    return one_hot\n",
    "\n",
    "def init_weights(input_size: int, output_size: int) -> np.ndarray:\n",
    "    # Normalized Xavier initialization\n",
    "    return np.random.uniform(-1, 1, (output_size, input_size)) * np.sqrt(6/(input_size + output_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining the LSTM class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 430,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM:\n",
    "    def __init__(self, input_size: int, hidden_size: int, output_size: int, learning_rate: float = 1e-3):\n",
    "        # Hyperparameters\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "        # Forget gate weights\n",
    "        self.W_f = init_weights(input_size, self.hidden_size)\n",
    "        self.b_f = np.zeros((self.hidden_size, 1))\n",
    "\n",
    "        # Input gate weights\n",
    "        self.W_i = init_weights(input_size, self.hidden_size)\n",
    "        self.b_i = np.zeros((self.hidden_size, 1))\n",
    "\n",
    "        # Candidate gate weights\n",
    "        self.W_c = init_weights(input_size, self.hidden_size)\n",
    "        self.b_c = np.zeros((self.hidden_size, 1))\n",
    "\n",
    "        # Output gate weights\n",
    "        self.W_o = init_weights(input_size, self.hidden_size)\n",
    "        self.b_o = np.zeros((self.hidden_size, 1))\n",
    "\n",
    "        # Final gate weights\n",
    "        self.W_y = init_weights(self.hidden_size, output_size)\n",
    "        self.b_y = np.zeros((output_size, 1))\n",
    "        \n",
    "        # Network cache\n",
    "        self.concat_inputs: dict[int, np.ndarray] = {}\n",
    "\n",
    "        self.hidden_states: dict[int, np.ndarray] = {-1: np.zeros((self.hidden_size, 1))}\n",
    "        self.cell_states: dict[int, np.ndarray] = {-1: np.zeros((self.hidden_size, 1))}\n",
    "\n",
    "        self.forget_gates: dict[int, np.ndarray] = {}\n",
    "        self.input_gates: dict[int, np.ndarray] = {}\n",
    "        self.candidate_gates: dict[int, np.ndarray] = {}\n",
    "        self.output_gates: dict[int, np.ndarray] = {}\n",
    "        self.activation_outputs: dict[int, np.ndarray] = {}\n",
    "        self.outputs: dict[int, np.ndarray] = {}\n",
    "        \n",
    "    def reset_cache(self):\n",
    "        self.concat_inputs = {}\n",
    "        \n",
    "        self.hidden_states = {-1: np.zeros((self.hidden_size, 1))}\n",
    "        self.cell_states = {-1: np.zeros((self.hidden_size, 1))}\n",
    "        \n",
    "        self.forget_gates = {}\n",
    "        self.input_gates = {}\n",
    "        self.candidate_gates = {}\n",
    "        self.output_gates = {}\n",
    "        self.activation_outputs = {}\n",
    "        self.outputs = {}\n",
    "        \n",
    "    def forward(self, inputs: list[np.ndarray]) -> list[np.ndarray]:\n",
    "        self.reset_cache()\n",
    "        \n",
    "        outputs = []\n",
    "        for t in range(len(inputs)):\n",
    "            self.concat_inputs[t] = np.concatenate((self.hidden_states[t-1], inputs[t]))\n",
    "            \n",
    "            self.forget_gates[t] = np.dot(self.W_f, self.concat_inputs[t]) + self.b_f\n",
    "            self.input_gates[t] = np.dot(self.W_i, self.concat_inputs[t]) + self.b_i\n",
    "            self.candidate_gates[t] = np.dot(self.W_c, self.concat_inputs[t]) + self.b_c\n",
    "            self.output_gates[t] = np.dot(self.W_o, self.concat_inputs[t]) + self.b_o\n",
    "            \n",
    "            fga = sigmoid(self.forget_gates[t])\n",
    "            iga = sigmoid(self.input_gates[t])\n",
    "            cga = tanh(self.candidate_gates[t])\n",
    "            oga = sigmoid(self.output_gates[t])\n",
    "            \n",
    "            self.cell_states[t] = fga * self.cell_states[t-1] + iga * cga\n",
    "            self.hidden_states[t] = oga * tanh(self.cell_states[t])\n",
    "            \n",
    "            outputs += [np.dot(self.W_y, self.hidden_states[t]) + self.b_y]\n",
    "            \n",
    "        return outputs\n",
    "    \n",
    "    def backward(self, errors: list[np.ndarray], inputs: list[np.ndarray]) -> None:\n",
    "        dW_f = 0\n",
    "        db_f = 0\n",
    "        \n",
    "        dW_i = 0\n",
    "        db_i = 0\n",
    "        \n",
    "        dW_c = 0\n",
    "        db_c = 0\n",
    "        \n",
    "        dW_o = 0\n",
    "        db_o = 0\n",
    "        \n",
    "        dW_y = 0\n",
    "        db_y = 0\n",
    "        \n",
    "        hidden_state = np.zeros_like(self.hidden_states[0])\n",
    "        cell_state = np.zeros_like(self.cell_states[0])\n",
    "        \n",
    "        for t in reversed(range(len(inputs))):\n",
    "            err = errors[t]\n",
    "            err = np.full((vocab_size, 1), err)\n",
    "            dW_y += np.dot(err, self.hidden_states[t].T)\n",
    "            db_y += err\n",
    "            \n",
    "            dhidden = np.dot(self.W_y.T, err) + hidden_state\n",
    "            doutput = tanh(self.cell_states[t]) * dhidden * dsigmoid(self.output_gates[t])\n",
    "            \n",
    "            dW_o += np.dot(doutput, self.concat_inputs[t].T)\n",
    "            db_o += doutput\n",
    "            \n",
    "            dcell_state = dtanh(self.cell_states[t]) * sigmoid(self.output_gates[t]) * dhidden + cell_state\n",
    "            \n",
    "            dforget = dcell_state * self.cell_states[t-1] * dsigmoid(self.forget_gates[t])\n",
    "            \n",
    "            dW_f += np.dot(dforget, self.concat_inputs[t].T)\n",
    "            db_f += dforget\n",
    "            \n",
    "            dinput = dcell_state * tanh(self.candidate_gates[t]) * dsigmoid(self.input_gates[t])\n",
    "            \n",
    "            dW_i += np.dot(dinput, self.concat_inputs[t].T)\n",
    "            db_i += dinput\n",
    "            \n",
    "            dcandidate = dcell_state * sigmoid(self.input_gates[t]) * dtanh(self.candidate_gates[t])\n",
    "            \n",
    "            dW_c += np.dot(dcandidate, self.concat_inputs[t].T)\n",
    "            db_c += dcandidate\n",
    "            \n",
    "            d_concat_inputs = np.dot(self.W_f.T, dforget) + np.dot(self.W_i.T, dinput) + np.dot(self.W_c.T, dcandidate) + np.dot(self.W_o.T, doutput)\n",
    "            \n",
    "            hidden_state = d_concat_inputs[:self.hidden_size, :]\n",
    "            cell_state = sigmoid(self.forget_gates[t]) * dcell_state\n",
    "            \n",
    "        for _d in (dW_f, db_f, dW_i, db_i, dW_c, db_c, dW_o, db_o, dW_y, db_y):\n",
    "            np.clip(_d, -1, 1, out=_d)\n",
    "            \n",
    "        self.W_f -= dW_f * self.learning_rate\n",
    "        self.b_f -= db_f * self.learning_rate\n",
    "        \n",
    "        self.W_i -= dW_i * self.learning_rate\n",
    "        self.b_i -= db_i * self.learning_rate\n",
    "        \n",
    "        self.W_c -= dW_c * self.learning_rate\n",
    "        self.b_c -= db_c * self.learning_rate\n",
    "        \n",
    "        self.W_o -= dW_o * self.learning_rate\n",
    "        self.b_o -= db_o * self.learning_rate\n",
    "        \n",
    "        self.W_y -= dW_y * self.learning_rate\n",
    "        self.b_y -= db_y * self.learning_rate\n",
    "        \n",
    "    def train(self, inputs: list[str], labels: list[str], epochs: int = 1):\n",
    "        one_hot_inputs = [one_hot_encode(c) for c in inputs]\n",
    "        \n",
    "        for _ in tqdm.tqdm(range(epochs)):\n",
    "            predictions = self.forward(one_hot_inputs)\n",
    "                \n",
    "            errors = []\n",
    "            for i in range(len(predictions)):\n",
    "                errors += [softmax(predictions[i])]\n",
    "                errors[-1][char_to_idx[labels[i]]] -= 1\n",
    "\n",
    "            self.backward(errors, self.concat_inputs)\n",
    "    \n",
    "    # Test\n",
    "    def test(self, inputs: list[str], labels: list[str]):\n",
    "        accuracy = 0\n",
    "        probabilities = self.forward([one_hot_encode(input) for input in inputs])\n",
    "\n",
    "        output = ''\n",
    "        for i in range(len(labels)):\n",
    "            prediction = idx_to_char[np.random.choice([*range(vocab_size)], p = softmax(probabilities[i].reshape(-1)))]\n",
    "\n",
    "            output += prediction\n",
    "\n",
    "            if prediction == labels[i]:\n",
    "                accuracy += 1\n",
    "\n",
    "        # print(f'Ground Truth:\\nt{labels}\\n')\n",
    "        print(f'Predictions:\\nt{\"\".join(output)}\\n')\n",
    "        \n",
    "        print(f'Accuracy: {round(accuracy * 100 / len(inputs), 2)}%')\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_size = 64\n",
    "input_size = vocab_size + hidden_size\n",
    "output_size = vocab_size\n",
    "\n",
    "learning_rate = 0.05"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Network initialization, training and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 431,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/500 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 1/500 [00:36<5:00:17, 36.11s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[431], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m lstm \u001b[38;5;241m=\u001b[39m LSTM(input_size, hidden_size, output_size, learning_rate\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.05\u001b[39m)\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m#### Training ####\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m \u001b[43mlstm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m500\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      5\u001b[0m lstm\u001b[38;5;241m.\u001b[39mtest(X_train, Y_train)\n",
      "Cell \u001b[1;32mIn[430], line 161\u001b[0m, in \u001b[0;36mLSTM.train\u001b[1;34m(self, inputs, labels, epochs)\u001b[0m\n\u001b[0;32m    158\u001b[0m     errors \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m [softmax(predictions[i])]\n\u001b[0;32m    159\u001b[0m     errors[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m][char_to_idx[labels[i]]] \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m--> 161\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconcat_inputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[430], line 122\u001b[0m, in \u001b[0;36mLSTM.backward\u001b[1;34m(self, errors, inputs)\u001b[0m\n\u001b[0;32m    119\u001b[0m dW_i \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mdot(dinput, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconcat_inputs[t]\u001b[38;5;241m.\u001b[39mT)\n\u001b[0;32m    120\u001b[0m db_i \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m dinput\n\u001b[1;32m--> 122\u001b[0m dcandidate \u001b[38;5;241m=\u001b[39m dcell_state \u001b[38;5;241m*\u001b[39m \u001b[43msigmoid\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minput_gates\u001b[49m\u001b[43m[\u001b[49m\u001b[43mt\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m*\u001b[39m dtanh(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcandidate_gates[t])\n\u001b[0;32m    124\u001b[0m dW_c \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mdot(dcandidate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconcat_inputs[t]\u001b[38;5;241m.\u001b[39mT)\n\u001b[0;32m    125\u001b[0m db_c \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m dcandidate\n",
      "Cell \u001b[1;32mIn[428], line 6\u001b[0m, in \u001b[0;36msigmoid\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdtanh\u001b[39m(x: np\u001b[38;5;241m.\u001b[39mndarray) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m np\u001b[38;5;241m.\u001b[39mndarray:\n\u001b[0;32m      4\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m np\u001b[38;5;241m.\u001b[39mtanh(x)\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m2\u001b[39m\n\u001b[1;32m----> 6\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msigmoid\u001b[39m(x: np\u001b[38;5;241m.\u001b[39mndarray) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m np\u001b[38;5;241m.\u001b[39mndarray:\n\u001b[0;32m      7\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;241m1\u001b[39m\u001b[38;5;241m/\u001b[39m(\u001b[38;5;241m1\u001b[39m\u001b[38;5;241m+\u001b[39mnp\u001b[38;5;241m.\u001b[39mexp(\u001b[38;5;241m-\u001b[39mx))\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdsigmoid\u001b[39m(x: np\u001b[38;5;241m.\u001b[39mndarray) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m np\u001b[38;5;241m.\u001b[39mndarray:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "lstm = LSTM(input_size, hidden_size, output_size, learning_rate)\n",
    "\n",
    "#### Training ####\n",
    "lstm.train(X_train, Y_train, epochs=500)\n",
    "lstm.test(X_train, Y_train)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
