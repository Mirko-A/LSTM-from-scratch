{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementing LSTM Neural Network for text generation\n",
    "LSTM: https://en.wikipedia.org/wiki/Long_short-term_memory\n",
    "\n",
    "Dataset: Tiny-Shakespeare <br>\n",
    "Link: https://github.com/karpathy/char-rnn/blob/master/data/tinyshakespeare/input.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import required modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = \"datasets/shakespeare/tiny_shakespeare_small.txt\"\n",
    "with open(dataset_path, 'r') as file:\n",
    "    data = file.read()\n",
    "\n",
    "print(f\"Data size: {len(data)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's create the vocabulary based on our dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#? NOTE: Mirko\n",
    "# Need to use dict.fromKeys to remove duplicates\n",
    "# since set() does not maintain the order of its\n",
    "# elements on each run.\n",
    "vocab = list(dict.fromkeys(data))\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "print(f\"Vocab size = {vocab_size}\")\n",
    "print(f\"Vocab      = {vocab}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We'll need to map characters to indices (and vice-versa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "char_to_idx = {c:i for i, c in enumerate(vocab)}\n",
    "idx_to_char = {i:c for i, c in enumerate(vocab)}\n",
    "\n",
    "a_idx = char_to_idx['a']\n",
    "print(f\"a-{a_idx}, {a_idx}-{idx_to_char[a_idx]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train/test split\n",
    "\n",
    "Next we'll split the dataset into the inputs (X) and the labels (Y). Since we're training the LSTM network to predict the next character in a sequence, our (input, label) pairs will be (current_char, next_char)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_size = len(data)\n",
    "X_train = data[:-1]\n",
    "Y_train = data[1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activation functions and their derivatives\n",
    "Tanh: https://en.wikipedia.org/wiki/Hyperbolic_functions#Tanh <br>\n",
    "Sigmoid: https://en.wikipedia.org/wiki/Sigmoid_function <br>\n",
    "Softmax: https://en.wikipedia.org/wiki/Softmax_function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tanh(x: np.ndarray) -> np.ndarray:\n",
    "    return np.tanh(x)\n",
    "def dtanh(x: np.ndarray) -> np.ndarray:\n",
    "    return 1 - np.tanh(x)**2\n",
    "\n",
    "def sigmoid(x: np.ndarray) -> np.ndarray:\n",
    "    return 1/(1+np.exp(-x))\n",
    "def dsigmoid(x: np.ndarray) -> np.ndarray:\n",
    "    return sigmoid(x)*(1-sigmoid(x))\n",
    "\n",
    "def softmax(x: np.ndarray) -> np.ndarray:\n",
    "    exp = np.exp(x)\n",
    "    return exp/np.sum(exp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss function\n",
    "\n",
    "Cross entropy loss: https://en.wikipedia.org/wiki/Cross-entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy_loss(y: np.ndarray, y_ref: np.ndarray) -> np.ndarray:\n",
    "    return -np.sum(y_ref * np.log(y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper functions\n",
    "One-hot encoding: https://en.wikipedia.org/wiki/One-hot <br>\n",
    "Weight initialization: https://machinelearningmastery.com/weight-initialization-for-deep-learning-neural-networks/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encode(c: str) -> np.ndarray:\n",
    "    one_hot = np.zeros((vocab_size, 1))\n",
    "    one_hot[char_to_idx[c]] = 1\n",
    "    \n",
    "    return one_hot\n",
    "\n",
    "def init_weights(input_size: int, output_size: int) -> np.ndarray:\n",
    "    # Normalized Xavier initialization\n",
    "    return np.random.uniform(-1, 1, (output_size, input_size)) * np.sqrt(6/(input_size + output_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining the LSTM class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM:\n",
    "    def __init__(self, input_size: int, hidden_size: int, output_size: int, learning_rate: float = 1e-3):\n",
    "        # Hyperparameters\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "        # Forget gate weights\n",
    "        self.W_f = init_weights(input_size, self.hidden_size)\n",
    "        self.b_f = np.zeros((self.hidden_size, 1))\n",
    "\n",
    "        # Input gate weights\n",
    "        self.W_i = init_weights(input_size, self.hidden_size)\n",
    "        self.b_i = np.zeros((self.hidden_size, 1))\n",
    "\n",
    "        # Candidate gate weights\n",
    "        self.W_c = init_weights(input_size, self.hidden_size)\n",
    "        self.b_c = np.zeros((self.hidden_size, 1))\n",
    "\n",
    "        # Output gate weights\n",
    "        self.W_o = init_weights(input_size, self.hidden_size)\n",
    "        self.b_o = np.zeros((self.hidden_size, 1))\n",
    "\n",
    "        # Final gate weights\n",
    "        self.W_y = init_weights(self.hidden_size, output_size)\n",
    "        self.b_y = np.zeros((output_size, 1))\n",
    "        \n",
    "        # Network cache\n",
    "        self.concat_inputs: dict[int, np.ndarray] = {}\n",
    "\n",
    "        self.hidden_states: dict[int, np.ndarray] = {-1: np.zeros((self.hidden_size, 1))}\n",
    "        self.cell_states: dict[int, np.ndarray] = {-1: np.zeros((self.hidden_size, 1))}\n",
    "\n",
    "        self.forget_gates: dict[int, np.ndarray] = {}\n",
    "        self.input_gates: dict[int, np.ndarray] = {}\n",
    "        self.candidate_gates: dict[int, np.ndarray] = {}\n",
    "        self.output_gates: dict[int, np.ndarray] = {}\n",
    "        self.activation_outputs: dict[int, np.ndarray] = {}\n",
    "        self.outputs: dict[int, np.ndarray] = {}\n",
    "        \n",
    "    def reset_cache(self):\n",
    "        self.concat_inputs = {}\n",
    "        \n",
    "        self.hidden_states = {-1: np.zeros((self.hidden_size, 1))}\n",
    "        self.cell_states = {-1: np.zeros((self.hidden_size, 1))}\n",
    "        \n",
    "        self.forget_gates = {}\n",
    "        self.input_gates = {}\n",
    "        self.candidate_gates = {}\n",
    "        self.output_gates = {}\n",
    "        self.activation_outputs = {}\n",
    "        self.outputs = {}\n",
    "        \n",
    "    def forward(self, inputs: list[np.ndarray]) -> list[np.ndarray]:\n",
    "        self.reset_cache()\n",
    "        \n",
    "        outputs = []\n",
    "        for t in range(len(inputs)):\n",
    "            self.concat_inputs[t] = np.concatenate((self.hidden_states[t-1], inputs[t]))\n",
    "            \n",
    "            self.forget_gates[t] = np.dot(self.W_f, self.concat_inputs[t]) + self.b_f\n",
    "            self.input_gates[t] = np.dot(self.W_i, self.concat_inputs[t]) + self.b_i\n",
    "            self.candidate_gates[t] = np.dot(self.W_c, self.concat_inputs[t]) + self.b_c\n",
    "            self.output_gates[t] = np.dot(self.W_o, self.concat_inputs[t]) + self.b_o\n",
    "            \n",
    "            fga = sigmoid(self.forget_gates[t])\n",
    "            iga = sigmoid(self.input_gates[t])\n",
    "            cga = tanh(self.candidate_gates[t])\n",
    "            oga = sigmoid(self.output_gates[t])\n",
    "            \n",
    "            self.cell_states[t] = fga * self.cell_states[t-1] + iga * cga\n",
    "            self.hidden_states[t] = oga * tanh(self.cell_states[t])\n",
    "            \n",
    "            output = softmax(np.dot(self.W_y, self.hidden_states[t]) + self.b_y)\n",
    "            outputs.append(output)\n",
    "            self.outputs[t] = output\n",
    "            \n",
    "        return outputs\n",
    "    \n",
    "    def backward(self, labels: list[np.ndarray]) -> None:\n",
    "        dW_f, db_f = 0, 0\n",
    "        dW_i, db_i = 0, 0\n",
    "        dW_c, db_c = 0, 0\n",
    "        dW_o, db_o = 0, 0\n",
    "        dW_y, db_y = 0, 0\n",
    "        \n",
    "        hidden_state = np.zeros_like(self.hidden_states[0])\n",
    "        cell_state = np.zeros_like(self.cell_states[0])\n",
    "        \n",
    "        for t in reversed(range(len(self.concat_inputs))):\n",
    "            dL = -(labels[t]/self.outputs[t])\n",
    "            dsoftmax = self.outputs[t] * dL - np.dot(self.outputs[t].T, dL) * self.outputs[t]\n",
    "            \n",
    "            dW_y += np.dot(dsoftmax, self.hidden_states[t].T)\n",
    "            db_y += dsoftmax\n",
    "            \n",
    "            dhidden_state = np.dot(self.W_y.T, dsoftmax) + hidden_state\n",
    "            doutput = tanh(self.cell_states[t]) * dhidden_state * dsigmoid(self.output_gates[t])\n",
    "            \n",
    "            dW_o += np.dot(doutput, self.concat_inputs[t].T)\n",
    "            db_o += doutput\n",
    "            \n",
    "            dcell_state = dtanh(self.cell_states[t]) * sigmoid(self.output_gates[t]) * dhidden_state + cell_state\n",
    "            \n",
    "            dforget = dcell_state * self.cell_states[t-1] * dsigmoid(self.forget_gates[t])\n",
    "            \n",
    "            dW_f += np.dot(dforget, self.concat_inputs[t].T)\n",
    "            db_f += dforget\n",
    "            \n",
    "            dinput = dcell_state * tanh(self.candidate_gates[t]) * dsigmoid(self.input_gates[t])\n",
    "            \n",
    "            dW_i += np.dot(dinput, self.concat_inputs[t].T)\n",
    "            db_i += dinput\n",
    "            \n",
    "            dcandidate = dcell_state * sigmoid(self.input_gates[t]) * dtanh(self.candidate_gates[t])\n",
    "            \n",
    "            dW_c += np.dot(dcandidate, self.concat_inputs[t].T)\n",
    "            db_c += dcandidate\n",
    "            \n",
    "            d_concat_inputs = np.dot(self.W_f.T, dforget) + np.dot(self.W_i.T, dinput) + np.dot(self.W_c.T, dcandidate) + np.dot(self.W_o.T, doutput)\n",
    "            \n",
    "            hidden_state = d_concat_inputs[:self.hidden_size, :]\n",
    "            cell_state = sigmoid(self.forget_gates[t]) * dcell_state\n",
    "            \n",
    "        for _d in (dW_f, db_f, dW_i, db_i, dW_c, db_c, dW_o, db_o, dW_y, db_y):\n",
    "            np.clip(_d, -1, 1, out=_d)\n",
    "            \n",
    "        self.W_f -= dW_f * self.learning_rate\n",
    "        self.b_f -= db_f * self.learning_rate\n",
    "        \n",
    "        self.W_i -= dW_i * self.learning_rate\n",
    "        self.b_i -= db_i * self.learning_rate\n",
    "        \n",
    "        self.W_c -= dW_c * self.learning_rate\n",
    "        self.b_c -= db_c * self.learning_rate\n",
    "        \n",
    "        self.W_o -= dW_o * self.learning_rate\n",
    "        self.b_o -= db_o * self.learning_rate\n",
    "        \n",
    "        self.W_y -= dW_y * self.learning_rate\n",
    "        self.b_y -= db_y * self.learning_rate\n",
    "        \n",
    "    def train(self, inputs: list[str], labels: list[str], epochs: int = 1) -> list[np.ndarray]:\n",
    "        one_hot_inputs = [one_hot_encode(c) for c in inputs]\n",
    "        one_hot_labels = [one_hot_encode(c) for c in labels]\n",
    "        \n",
    "        losses = []\n",
    "        for _ in tqdm.tqdm(range(epochs)):\n",
    "            predictions = self.forward(one_hot_inputs)\n",
    "            N = len(predictions)\n",
    "                \n",
    "            loss = 0\n",
    "            for i in range(N):\n",
    "                loss += cross_entropy_loss(predictions[i], one_hot_labels[i])\n",
    "\n",
    "            losses.append(loss/N)\n",
    "            self.backward(one_hot_labels)\n",
    "            \n",
    "        return losses\n",
    "    \n",
    "    def test(self, inputs: list[str], labels: list[str]):\n",
    "        accuracy = 0\n",
    "        probabilities = self.forward([one_hot_encode(input) for input in inputs])\n",
    "\n",
    "        output = ''\n",
    "        for i in range(len(labels)):\n",
    "            prediction = idx_to_char[np.random.choice(vocab_size, p = probabilities[i].reshape(-1))]\n",
    "\n",
    "            output += prediction\n",
    "\n",
    "            if prediction == labels[i]:\n",
    "                accuracy += 1\n",
    "\n",
    "        print(f'Predictions:\\nt{\"\".join(output)}\\n')\n",
    "        print(f'Accuracy: {round(accuracy * 100 / len(inputs), 2)}%')\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_size = 64\n",
    "input_size = vocab_size + hidden_size\n",
    "output_size = vocab_size\n",
    "\n",
    "learning_rate = 0.06\n",
    "epochs = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Network initialization and training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm = LSTM(input_size, hidden_size, output_size, learning_rate)\n",
    "losses = lstm.train(X_train, Y_train, epochs)\n",
    "\n",
    "losses = [loss.item() for loss in losses]\n",
    "plt.plot(losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm.test(X_train, Y_train)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
